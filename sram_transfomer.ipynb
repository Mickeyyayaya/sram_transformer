{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SRAMSimulator:\n",
    "    def __init__(self, sram_sizes):\n",
    "        self.sram = {'input': None, 'weight': None, 'output': None,\n",
    "                     'input_2': None, 'weight_2': None, 'output_2': None,\n",
    "                     'input_3': None, 'weight_3': None, 'output_3': None}\n",
    "        self.sram_sizes = sram_sizes  # Now it's a dictionary that holds sizes for each SRAM segment\n",
    "        self.transfer_counts = {'load_total': 0, 'clear_total': 0, 'input_load': 0, 'input_clear': 0,\n",
    "                                'weight_load': 0, 'weight_clear': 0, 'output_load': 0, 'output_clear': 0}\n",
    "\n",
    "    def load_to_sram(self, name, data ,test_num):\n",
    "        # Calculate the size of the new data to be added\n",
    "        data_size = data.numel() * data.element_size()\n",
    "        temp_out = None\n",
    "        # Retrieve existing data if it exists, or initialize an empty placeholder\n",
    "        existing_data = self.sram.get(name, None)\n",
    "        if existing_data is not None:\n",
    "            # Calculate the new total size after appending\n",
    "            total_data_size = existing_data.numel() * existing_data.element_size() + data_size\n",
    "        else:\n",
    "            total_data_size = data_size\n",
    "        \n",
    "        # Check if the SRAM can accommodate the new total size of data\n",
    "        if total_data_size > self.sram_sizes[name]:\n",
    "            temp_out = self.test_output(name)\n",
    "            self.clear_sram(name)\n",
    "            existing_data = None\n",
    "            \n",
    "\n",
    "        if existing_data is not None:\n",
    "            if name == 'input':\n",
    "                self.sram[name] = torch.cat((existing_data, data), dim=0)\n",
    "            elif name == 'weight':\n",
    "                self.sram[name] = torch.cat((existing_data, data), dim=1)\n",
    "            elif name == 'output':\n",
    "               self.sram[name] = torch.cat((existing_data, data), dim=0)\n",
    "        else:\n",
    "            self.sram[name] = data\n",
    "\n",
    "        # Update the transfer counts\n",
    "        self.transfer_counts['load_total'] += 1\n",
    "        self.transfer_counts[f'{name}_load'] += 1\n",
    "      #  print(temp_out)\n",
    "        return temp_out\n",
    "    def test_output(self,name):\n",
    "        out = self.sram[name]\n",
    "        return out\n",
    "    def clear_sram(self, name):\n",
    "       # print(name+\"clear\")\n",
    "        self.sram[name] = None\n",
    "        self.transfer_counts['clear_total'] += 1\n",
    "        self.transfer_counts[f'{name}_clear'] += 1\n",
    "      #  print(f\"Cleared {name} from SRAM. Transfer counts: {self.transfer_counts}\")\n",
    "\n",
    "    def calculate_attention(self, embedding, wq, wk, wv, test_num):\n",
    "      \n",
    "        self.load_to_sram('input',embedding,test_num)\n",
    "\n",
    "        # Calculate Q\n",
    "        self.load_to_sram('weight', wq,test_num)\n",
    "        q = torch.matmul(self.sram['input'], self.sram['weight'])\n",
    "        self.clear_sram('weight')\n",
    "        self.load_to_sram('output',q,test_num)\n",
    "        self.clear_sram('output')\n",
    "\n",
    "        \n",
    "        # Calculate K\n",
    "        self.load_to_sram('weight', wk,test_num) \n",
    "        k = torch.matmul(self.sram['input'], self.sram['weight'])\n",
    "        self.clear_sram('weight')\n",
    "        self.load_to_sram('output',k,test_num)\n",
    "        self.clear_sram('output')\n",
    "\n",
    "        # Calculate V\n",
    "        self.load_to_sram('weight', wv,test_num)\n",
    "        v = torch.matmul(self.sram['input'], self.sram['weight'])\n",
    "        self.clear_sram('weight')\n",
    "        self.load_to_sram('output',v,test_num)\n",
    "        self.clear_sram('output')\n",
    "\n",
    "\n",
    "        self.clear_sram('input')\n",
    "        # Calculate Attention Scores\n",
    "        k_t = k.transpose(-2, -1)\n",
    "        self.load_to_sram('weight', k_t,test_num)\n",
    "        self.load_to_sram('input', q,test_num)\n",
    "        attn_scores = torch.matmul(self.sram['input'], self.sram['weight'])\n",
    "        self.clear_sram('input')\n",
    "        self.clear_sram('weight')\n",
    "        self.load_to_sram('output',attn_scores,test_num)\n",
    "        self.clear_sram('output')\n",
    "\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores / (k.size(-1) ** 0.5), dim=-1)\n",
    "\n",
    "        # Calculate Attention Output\n",
    "        self.load_to_sram('input', attn_weights,test_num)\n",
    "        self.load_to_sram('weight', v,test_num)\n",
    "        attn_output = torch.matmul(self.sram['input'], self.sram['weight'])\n",
    "        self.clear_sram('input')\n",
    "        self.clear_sram('weight')\n",
    "        self.load_to_sram('output', attn_output,test_num)\n",
    "        self.clear_sram('output')\n",
    "\n",
    "        output_size = attn_output.numel() * attn_output.element_size()\n",
    "        print(f\"Final output size: {output_size} bytes\")\n",
    "\n",
    "        \n",
    "        return attn_output\n",
    "\n",
    "    def get_transfer_count(self, name='total'):\n",
    "        return self.transfer_counts[name]\n",
    "\n",
    "    def cal_att2(self, embedding, wq, wk, wv, test_num, dimension, max_seq):\n",
    "        #print(\"====================== Word Embedding ======================\")\n",
    "        Q = None\n",
    "        K = None\n",
    "        V = None\n",
    "        att_u = None\n",
    "        final_out = None\n",
    "        for i in range(0,max_seq,test_num):\n",
    "            clear_input = self.load_to_sram('input',embedding[0][i:i+test_num],test_num)\n",
    "            counter = 0\n",
    "            for j in range(0,dimension,test_num):\n",
    "                clear_weight = self.load_to_sram('weight',wq[:,j:j+test_num],test_num)\n",
    "                counter2 = 0\n",
    "                if i == 0 and j == 0:\n",
    "                    # Assuming that existing_data and data are both PyTorch tensors\n",
    "                    # We need to concatenate along a specific dimension (0 by default)\n",
    "                    q2 = torch.matmul(self.sram['input'],self.sram['weight'])\n",
    "\n",
    "                elif clear_input != None and clear_weight != None:\n",
    "                    q2 = torch.matmul(self.sram['input'],self.sram['weight'])\n",
    "                    counter = 0\n",
    "\n",
    "                elif clear_input != None and clear_weight == None:\n",
    "                    q2 = torch.matmul(self.sram['input'],self.sram['weight'][:,counter2:counter2+test_num])\n",
    "                    counter = 0\n",
    "                 \n",
    "                elif clear_input == None and clear_weight != None:\n",
    "                    q2 = torch.matmul(self.sram['input'][counter:counter+test_num],self.sram['weight'])\n",
    "                  \n",
    "                else:\n",
    "                    q2 = torch.matmul(self.sram['input'][counter:counter+test_num],self.sram['weight'][:,counter2:counter2+test_num])\n",
    "                   \n",
    "                a = self.load_to_sram('output',q2,test_num)\n",
    "                counter2 += 1\n",
    "                if a!= None and Q == None:\n",
    "                    Q = a\n",
    "                elif a != None and Q != None :\n",
    "                    Q = torch.cat((Q,a),dim = 0)\n",
    "            counter += 1\n",
    "        #Q = self.sram['output']\n",
    "        if Q == None:\n",
    "            Q = self.sram['output']\n",
    "        elif Q.shape[0] != dimension*max_seq/2:\n",
    "            Q = torch.cat((Q,self.sram['output']),dim = 0)\n",
    "        Q = Q.view(max_seq,dimension)\n",
    "        #print(Q.shape)\n",
    "        self.clear_sram('input')\n",
    "        self.clear_sram('weight')\n",
    "        self.clear_sram('output')\n",
    "\n",
    "        for i in range(0,max_seq,test_num):\n",
    "            input = embedding[0][i:i+test_num]\n",
    "            counter = 0\n",
    "            for j in range(0,dimension,test_num):\n",
    "                clear_weight = self.load_to_sram('weight',wv[:,j:j+test_num],test_num)\n",
    "                counter2 = 0\n",
    "                if i == 0 and j == 0:\n",
    "                    # Assuming that existing_data and data are both PyTorch tensors\n",
    "                    # We need to concatenate along a specific dimension (0 by default)\n",
    "                    v2 = torch.matmul(input,self.sram['weight'])\n",
    "\n",
    "                elif clear_input != None and clear_weight != None:\n",
    "                    v2 = torch.matmul(input,self.sram['weight'])\n",
    "                    counter = 0\n",
    "\n",
    "                elif clear_input != None and clear_weight == None:\n",
    "                    v2 = torch.matmul(input,self.sram['weight'][:,counter2:counter2+test_num])\n",
    "                    counter = 0\n",
    "                 \n",
    "                elif clear_input == None and clear_weight != None:\n",
    "                    v2 = torch.matmul(input,self.sram['weight'])\n",
    "                  \n",
    "                else:\n",
    "                    v2 = torch.matmul(input,self.sram['weight'][:,counter2:counter2+test_num])\n",
    "\n",
    "                a = self.load_to_sram('output',v2,test_num)\n",
    "                counter2 += 1\n",
    "                if a!= None and V == None:\n",
    "                    V = a\n",
    "                elif a != None and V != None :\n",
    "                    V = torch.cat((V,a),dim = 0)\n",
    "        #Q = self.sram['output']\n",
    "        if V == None:\n",
    "            V = self.sram['output']\n",
    "        elif V.shape[0] != dimension*max_seq/2:\n",
    "            V = torch.cat((V,self.sram['output']),dim = 0)\n",
    "        V = V.view(max_seq,dimension)\n",
    "        self.clear_sram('input')\n",
    "        self.clear_sram('weight')\n",
    "        self.clear_sram('output')\n",
    "\n",
    "        for i in range(0,max_seq,test_num):\n",
    "            input = embedding[0][i:i+test_num]\n",
    "            counter = 0\n",
    "            for j in range(0,dimension,test_num):\n",
    "                clear_weight = self.load_to_sram('weight',wk[:,j:j+test_num],test_num)\n",
    "                counter2 = 0\n",
    "                if i == 0 and j == 0:\n",
    "                    # Assuming that existing_data and data are both PyTorch tensors\n",
    "                    # We need to concatenate along a specific dimension (0 by default)\n",
    "                    k2 = torch.matmul(input,self.sram['weight'])\n",
    "\n",
    "                elif clear_input != None and clear_weight != None:\n",
    "                    k2 = torch.matmul(input,self.sram['weight'])\n",
    "                    counter = 0\n",
    "\n",
    "                elif clear_input != None and clear_weight == None:\n",
    "                    k2 = torch.matmul(input,self.sram['weight'][:,counter2:counter2+test_num])\n",
    "                    counter = 0\n",
    "                 \n",
    "                elif clear_input == None and clear_weight != None:\n",
    "                    k2 = torch.matmul(input,self.sram['weight'])\n",
    "                  \n",
    "                else:\n",
    "                    k2 = torch.matmul(input,self.sram['weight'][:,counter2:counter2+test_num])\n",
    "                a = self.load_to_sram('output',k2,test_num)\n",
    "                counter2 += 1\n",
    "                if a!= None and K == None:\n",
    "                    K = a\n",
    "                elif a != None and K != None :\n",
    "                    K = torch.cat((K,a),dim = 0)\n",
    "        if K == None:\n",
    "            K = self.sram['output']\n",
    "        elif K.shape[0] != dimension*max_seq/2:\n",
    "            K = torch.cat((K,self.sram['output']),dim = 0)\n",
    "\n",
    "        K = K.view(max_seq,dimension)\n",
    "        self.clear_sram('input')\n",
    "        self.clear_sram('weight')\n",
    "        self.clear_sram('output')\n",
    "        k_t = K.transpose(-2,-1)\n",
    "       # print(k_t.shape)\n",
    "\n",
    "        #print(\"====================== qkt ======================\")\n",
    "        #print(Q.shape)\n",
    "        #print(k_t.shape)\n",
    "        for i in range(0, max_seq, test_num):\n",
    "            clear_input = self.load_to_sram('input', Q[i:i+test_num], test_num)\n",
    "            counter = 0\n",
    "            for j in range(0, max_seq, test_num):\n",
    "                clear_weight = self.load_to_sram('weight', k_t[:,j:j+test_num], test_num)\n",
    "                counter2 = 0\n",
    "                if i == 0 and j == 0:\n",
    "                    # Assuming that existing_data and data are both PyTorch tensors\n",
    "                    # We need to concatenate along a specific dimension (0 by default)\n",
    "                    att = torch.matmul(self.sram['input'],self.sram['weight'])\n",
    "\n",
    "                elif clear_input != None and clear_weight != None:\n",
    "                    att = torch.matmul(self.sram['input'],self.sram['weight'])\n",
    "                    counter = 0\n",
    "\n",
    "                elif clear_input != None and clear_weight == None:\n",
    "                    att = torch.matmul(self.sram['input'],self.sram['weight'][:,counter2 :counter2+test_num])\n",
    "                    counter = 0\n",
    "                 \n",
    "                elif clear_input == None and clear_weight != None:\n",
    "                    att = torch.matmul(self.sram['input'][counter:counter+test_num],self.sram['weight'])\n",
    "                  \n",
    "                else:\n",
    "                    att = torch.matmul(self.sram['input'][counter:counter+test_num],self.sram['weight'][:,counter2:counter2+test_num])\n",
    "\n",
    "                a = self.load_to_sram('output',att,test_num)\n",
    "                counter2 += 1\n",
    "                if a!= None and att_u == None:\n",
    "                    att_u = a\n",
    "                elif a != None and att != None :\n",
    "                    att_u = torch.cat((att_u,a),dim = 0)\n",
    "            self.clear_sram('weight')\n",
    "        if att_u == None:\n",
    "            att_u = self.sram['output']\n",
    "        elif att_u.shape[0] != dimension*max_seq/2:\n",
    "            att_u = torch.cat((att_u,self.sram['output']),dim = 0)\n",
    "        att_u = att_u.view(max_seq,max_seq)\n",
    "        #print(att_u.shape)\n",
    "        attn_weights = F.softmax(att_u/ (K.size(-1) ** 0.5), dim=-1)\n",
    "       # print(attn_weights.shape)\n",
    "        self.clear_sram('weight')\n",
    "        self.clear_sram('input')\n",
    "        self.clear_sram('output')\n",
    "\n",
    "       # print(\"====================== attention ======================\")\n",
    "        #print(attn_weights.shape)\n",
    "       # print(V.shape)\n",
    "        for i in range(0, max_seq, test_num):\n",
    "            clear_input = self.load_to_sram('input', attn_weights[i:i+test_num], test_num)\n",
    "            counter = 0\n",
    "            for j in range(0, dimension, test_num):\n",
    "                counter2 = 0\n",
    "                clear_weight = self.load_to_sram('weight', V[:,j:j+test_num], test_num)\n",
    "                if i == 0 and j == 0:\n",
    "                    # Assuming that existing_data and data are both PyTorch tensors\n",
    "                    # We need to concatenate along a specific dimension (0 by default)\n",
    "                    out = torch.matmul(self.sram['input'],self.sram['weight'])\n",
    "\n",
    "                elif clear_input != None and clear_weight != None:\n",
    "                    out = torch.matmul(self.sram['input'],self.sram['weight'])\n",
    "                    counter = 0\n",
    "\n",
    "                elif clear_input != None and clear_weight == None:\n",
    "                    out = torch.matmul(self.sram['input'],self.sram['weight'][:,counter2:counter2+test_num])\n",
    "                    counter = 0\n",
    "                 \n",
    "                elif clear_input == None and clear_weight != None:\n",
    "                    out = torch.matmul(self.sram['input'][counter:counter+test_num],self.sram['weight'])\n",
    "                  \n",
    "                else:\n",
    "                    out = torch.matmul(self.sram['input'][counter:counter+test_num],self.sram['weight'][:,counter2:counter2+test_num])\n",
    "                \n",
    "                a = self.load_to_sram('output',out,test_num)\n",
    "                counter2 += 1\n",
    "                if a!= None and final_out == None:\n",
    "                    final_out = a\n",
    "                elif a != None and final_out != None :\n",
    "                    final_out = torch.cat((final_out,a),dim = 0)\n",
    "            self.clear_sram('weight')\n",
    "            \n",
    "        if final_out == None:\n",
    "            final_out = self.sram['output']\n",
    "        elif final_out.shape[0] != dimension*max_seq/2:\n",
    "            final_out = torch.cat((final_out,self.sram['output']),dim = 0)\n",
    "        final_out = final_out.view(max_seq,dimension)\n",
    "        #print(final_out.shape)\n",
    "        self.clear_sram('output')\n",
    "        print(\"Finish\") \n",
    "\n",
    "    def cal_att3(self, embedding, wq, wk, wv, test_num, dimension, max_seq):\n",
    "        print(\"====================== Word Embedding ======================\")\n",
    "        Q = None\n",
    "        K = None\n",
    "        V = None\n",
    "        Att = None\n",
    "        final_out = None\n",
    "        for i in range(0,max_seq,test_num):\n",
    "            clear_input = self.load_to_sram('input',embedding[0][i:i+test_num],test_num)\n",
    "            counter = 0\n",
    "            counter2 = 0\n",
    "            for j in range(0,dimension,test_num):\n",
    "                clear_weight_Q = self.load_to_sram('weight',wq[:,j:j+test_num],test_num)\n",
    "                clear_weight_K = self.load_to_sram('weight',wk[:,j:j+test_num],test_num)\n",
    "                clear_weight_V = self.load_to_sram('weight',wv[:,j:j+test_num],test_num)\n",
    "\n",
    "                if clear_weight_K != None:\n",
    "                    #print('b')\n",
    "                    temp_q = torch.matmul(self.sram['input'][i:i+test_num],clear_weight_K[:,counter2:counter2+test_num])\n",
    "                    counter2 = 0\n",
    "                    temp_k = torch.matmul(self.sram['input'][i:i+test_num],self.sram['weight'][:,counter2:counter2+test_num])\n",
    "                    temp_v = torch.matmul(self.sram['input'][i:i+test_num],self.sram['weight'][:,counter2+2:counter2+test_num+2])\n",
    "                    counter2 += 4\n",
    "                elif clear_weight_V != None:\n",
    "                    #print('c')\n",
    "                    temp_k = torch.matmul(self.sram['input'][i:i+test_num],clear_weight_V[:,counter2:counter2+test_num])\n",
    "                    temp_q = torch.matmul(self.sram['input'][i:i+test_num],clear_weight_V[:,counter2-2:counter2+test_num-2])\n",
    "                    counter2 = 0\n",
    "                    temp_v = torch.matmul(self.sram['input'][i:i+test_num],self.sram['weight'][:,counter2:counter2+test_num])\n",
    "                    counter2 += 2\n",
    "                else:\n",
    "                    #print('d')\n",
    "                    temp_q = torch.matmul(self.sram['input'][i:i+test_num],self.sram['weight'][:,counter2:counter2+test_num])\n",
    "                    temp_k = torch.matmul(self.sram['input'][i:i+test_num],self.sram['weight'][:,counter2+2:counter2+test_num+2])\n",
    "                    temp_v = torch.matmul(self.sram['input'][i:i+test_num],self.sram['weight'][:,counter2+4:counter2+test_num+4])\n",
    "                    counter2 += 6          \n",
    "                if i == 0 and j == 0:\n",
    "                    Q = temp_q\n",
    "                    K = temp_k\n",
    "                    V = temp_v\n",
    "                else:\n",
    "                    Q = torch.cat((Q,temp_q),dim = 0)\n",
    "                    K = torch.cat((K,temp_k),dim = 0)\n",
    "                    V = torch.cat((V,temp_v),dim = 0)\n",
    "                \n",
    "            print(Q.shape)\n",
    "            print(K.shape)\n",
    "            print(V.shape)\n",
    "            Q.view(max_seq,dimension)\n",
    "            V.view(max_seq,dimension)\n",
    "            K.view(max_seq,dimension)\n",
    "            \n",
    "            k_t = K.transpose(-2,-1)\n",
    "            \n",
    "            print(\"====================== qkt ======================\")\n",
    "            for i in range(0, max_seq, test_num):\n",
    "                clear_input = self.load_to_sram('input', Q[i:i+test_num], test_num)\n",
    "                counter2 =  0\n",
    "                for j in range(0, max_seq, test_num):\n",
    "                    clear_weight = self.load_to_sram('weight', k_t[:,j:j+test_num], test_num)\n",
    "                    counter2 = 0\n",
    "                    if i == 0 and j == 0:\n",
    "                        # Assuming that existing_data and data are both PyTorch tensors\n",
    "                        # We need to concatenate along a specific dimension (0 by default)\n",
    "                        att = torch.matmul(self.sram['input'],self.sram['weight'])\n",
    "\n",
    "                    elif clear_input != None and clear_weight != None:\n",
    "                        att = torch.matmul(self.sram['input'],self.sram['weight'])\n",
    "                        counter = 0\n",
    "\n",
    "                    elif clear_input != None and clear_weight == None:\n",
    "                        att = torch.matmul(self.sram['input'],self.sram['weight'][:,counter2 :counter2+test_num])\n",
    "                        counter = 0\n",
    "                    \n",
    "                    elif clear_input == None and clear_weight != None:\n",
    "                        att = torch.matmul(self.sram['input'][counter:counter+test_num],self.sram['weight'])\n",
    "                    \n",
    "                    else:\n",
    "                        att = torch.matmul(self.sram['input'][counter:counter+test_num],self.sram['weight'][:,counter2:counter2+test_num])\n",
    "\n",
    "                    a = self.load_to_sram('output',att,test_num)\n",
    "                    counter2 += 1\n",
    "                    if a!= None and att_u == None:\n",
    "                        att_u = a\n",
    "                    elif a != None and att != None :\n",
    "                        att_u = torch.cat((att_u,a),dim = 0)\n",
    "                self.clear_sram('weight')\n",
    "            if att_u == None:\n",
    "                att_u = self.sram['output']\n",
    "            elif att_u.shape[0] != dimension*max_seq/2:\n",
    "                att_u = torch.cat((att_u,self.sram['output']),dim = 0)\n",
    "            att_u = att_u.view(max_seq,max_seq)\n",
    "            print(att_u.shape)\n",
    "            attn_weights = F.softmax(att_u/ (K.size(-1) ** 0.5), dim=-1)\n",
    "            print(attn_weights.shape)\n",
    "            self.clear_sram('weight')\n",
    "            self.clear_sram('input')\n",
    "            self.clear_sram('output')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Word Embedding ======================\n",
      "torch.Size([512, 2])\n",
      "torch.Size([512, 2])\n",
      "torch.Size([512, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[100, 512]' is invalid for input of size 1024",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mickey\\sram_transformer\\sram_transfomer.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mickey/sram_transformer/sram_transfomer.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m wk \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(dimension, dimension)  \u001b[39m# Example weight for K\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mickey/sram_transformer/sram_transfomer.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m wv \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(dimension, dimension)  \u001b[39m# Example weight for V\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mickey/sram_transformer/sram_transfomer.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m attn_output \u001b[39m=\u001b[39m simulator\u001b[39m.\u001b[39;49mcal_att3(embedding, wq, wk, wv, test_num, dimension, max_seq)\n",
      "\u001b[1;32mc:\\Users\\mickey\\sram_transformer\\sram_transfomer.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mickey/sram_transformer/sram_transfomer.ipynb#W1sZmlsZQ%3D%3D?line=386'>387</a>\u001b[0m \u001b[39mprint\u001b[39m(K\u001b[39m.\u001b[39mshape)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mickey/sram_transformer/sram_transfomer.ipynb#W1sZmlsZQ%3D%3D?line=387'>388</a>\u001b[0m \u001b[39mprint\u001b[39m(V\u001b[39m.\u001b[39mshape)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/mickey/sram_transformer/sram_transfomer.ipynb#W1sZmlsZQ%3D%3D?line=388'>389</a>\u001b[0m Q\u001b[39m.\u001b[39;49mview(max_seq,dimension)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mickey/sram_transformer/sram_transfomer.ipynb#W1sZmlsZQ%3D%3D?line=389'>390</a>\u001b[0m V\u001b[39m.\u001b[39mview(max_seq,dimension)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mickey/sram_transformer/sram_transfomer.ipynb#W1sZmlsZQ%3D%3D?line=390'>391</a>\u001b[0m K\u001b[39m.\u001b[39mview(max_seq,dimension)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[100, 512]' is invalid for input of size 1024"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "dimension = 512\n",
    "max_seq = 100\n",
    "test_num = 2\n",
    "sram_sizes = {  \n",
    "    'input':  10000* max_seq * 4,  # SRAM size for input\n",
    "    'weight': dimension * dimension * 4,  # SRAM size for weights\n",
    "    'output': dimension * dimension * 4   # SRAM size for output\n",
    "}\n",
    "simulator = SRAMSimulator(sram_sizes)\n",
    "\n",
    "embedding = torch.randn( 1, max_seq, dimension)  # Example input embedding\n",
    "wq = torch.randn(dimension, dimension)  # Example weight for Q\n",
    "wk = torch.randn(dimension, dimension)  # Example weight for K\n",
    "wv = torch.randn(dimension, dimension)  # Example weight for V\n",
    "attn_output = simulator.cal_att3(embedding, wq, wk, wv, test_num, dimension, max_seq)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'load_total': 262, 'clear_total': 1, 'input_load': 1, 'input_clear': 0, 'weight_load': 261, 'weight_clear': 1, 'output_load': 0, 'output_clear': 0}\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n",
      "Finish\n"
     ]
    }
   ],
   "source": [
    "dimension = 512\n",
    "max_seq = 100\n",
    "test_num = 2\n",
    "import csv\n",
    "\n",
    "head = ['buffer_size', 'load_total', 'clear_total', 'input_load', 'input_clear', 'weight_load', 'weight_clear', 'output_load', 'output_clear']\n",
    "\n",
    "for sram_buffer in range(1024,262144,10240):\n",
    "    sram_sizes = {  \n",
    "        'input':   sram_buffer * 4,  # SRAM size for input\n",
    "        'weight':  sram_buffer * 4,  # SRAM size for weights\n",
    "        'output':  sram_buffer * 4   # SRAM size for output\n",
    "    }\n",
    "    simulator = SRAMSimulator(sram_sizes)\n",
    "\n",
    "    embedding = torch.randn( 1, max_seq, dimension)  # Example input embedding\n",
    "    wq = torch.randn(dimension, dimension)  # Example weight for Q\n",
    "    wk = torch.randn(dimension, dimension)  # Example weight for K\n",
    "    wv = torch.randn(dimension, dimension)  # Example weight for V\n",
    "    attn_output = simulator.cal_att2(embedding, wq, wk, wv, test_num, dimension, max_seq)\n",
    "    if sram_buffer == 1024:\n",
    "        data = [head,[sram_buffer,simulator.transfer_counts['load_total'],simulator.transfer_counts['clear_total'],simulator.transfer_counts['input_load'],\n",
    "                simulator.transfer_counts['input_clear'],simulator.transfer_counts['weight_load'],simulator.transfer_counts['weight_clear'],\n",
    "                simulator.transfer_counts['output_load'],simulator.transfer_counts['output_clear']]]   \n",
    "    else:\n",
    "        data = [[sram_buffer,simulator.transfer_counts['load_total'],simulator.transfer_counts['clear_total'],simulator.transfer_counts['input_load'],\n",
    "                simulator.transfer_counts['input_clear'],simulator.transfer_counts['weight_load'],simulator.transfer_counts['weight_clear'],\n",
    "                simulator.transfer_counts['output_load'],simulator.transfer_counts['output_clear']]]\n",
    "    with open('outputt.csv', mode= 'a+' , newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data)\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simulator.transfer_counts['load_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base SRAM sizes\n",
    "base_sram_sizes = {  \n",
    "    'input': 512 * 100 * 4,\n",
    "    'weight': 512 * 512 * 4,\n",
    "    'output': 512 * 512 * 4\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Define the factors by which to multiply each SRAM size to create different configurations\n",
    "factors = [0.5, 1, 2, 4]\n",
    "\n",
    "# Generate combinations of SRAM sizes\n",
    "sram_combinations = []\n",
    "for input_factor in factors:\n",
    "    for weight_factor in factors:\n",
    "        for output_factor in factors:\n",
    "            sram_combinations.append({\n",
    "                'input': base_sram_sizes['input'] * input_factor,\n",
    "                'weight': base_sram_sizes['weight'] * weight_factor,\n",
    "                'output': base_sram_sizes['output'] * output_factor\n",
    "            })\n",
    "\n",
    "# Now sram_combinations contains all the different SRAM size configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "dimension = 512\n",
    "max_seq = 100\n",
    "test_num = 2\n",
    "sram_sizes = {  \n",
    "    'input':  dimension* max_seq * 4,  # SRAM size for input\n",
    "    'weight': dimension * dimension * 4,  # SRAM size for weights\n",
    "    'output': dimension * dimension * 4   # SRAM size for output\n",
    "}\n",
    "simulator = SRAMSimulator(sram_sizes)\n",
    "\n",
    "embedding = torch.randn( 1, max_seq, dimension)  # Example input embedding\n",
    "wq = torch.randn(dimension, dimension)  # Example weight for Q\n",
    "wk = torch.randn(dimension, dimension)  # Example weight for K\n",
    "wv = torch.randn(dimension, dimension)  # Example weight for V\n",
    "attn_output = simulator.cal_att2(embedding, wq, wk, wv, test_num, dimension, max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
